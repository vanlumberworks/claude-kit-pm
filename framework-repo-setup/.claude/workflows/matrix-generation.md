# Matrix Generation Workflow

## Visual Decision Frameworks and Comparison Analysis

This workflow provides systematic approaches for creating comparison matrices, decision frameworks, and analytical tables that transform complex information into clear, actionable visual formats.

## Overview

**Purpose**: Create visual tools that simplify complex comparisons and facilitate data-driven decisions

**When to use**:
- Comparing multiple options, features, or competitors
- Making multi-criteria decisions
- Synthesizing research from multiple sources
- Building stakeholder alignment through visualization
- Prioritizing features or initiatives
- Assessing and communicating risks
- Tracking metrics and progress

**Outputs**:
- Feature comparison matrices
- Multi-criteria decision matrices (MCDA)
- Research synthesis matrices
- Stakeholder alignment matrices
- Prioritization matrices
- Risk assessment matrices

## Matrix Type Selection Guide

**Choose your matrix type based on your need**:

| Need | Matrix Type | Best For |
|------|-------------|----------|
| Compare features across products | Feature Comparison | Competitive analysis, gap identification |
| Evaluate solution options | MCDA Decision Matrix | Build vs. buy, vendor selection |
| Synthesize research | Research Synthesis | Multi-source findings, confidence scoring |
| Align stakeholders | Stakeholder Position | Consensus building, conflict resolution |
| Prioritize work | Prioritization Matrix | Backlog management, roadmap planning |
| Assess risks | Risk Assessment | Risk management, mitigation planning |
| Track progress | Metrics Dashboard | KPI monitoring, status reporting |

## Workflow 1: Feature Comparison Matrix

### Use Case
Comparing features across your product and competitors to identify gaps and opportunities.

### Step 1: Define Comparison Scope
**Goal**: Identify what features to compare and against whom

**Inputs Needed**:
- Your product feature list
- Competitor list (typically 3-5)
- Feature categories to evaluate
- Market expectations/trends

**Template**:
```markdown
## Feature Comparison Scope
**Products to Compare**:
- Us: [Your product]
- Competitor 1: [Name]
- Competitor 2: [Name]
- Competitor 3: [Name]

**Feature Categories**:
1. [Category 1] - e.g., Authentication
2. [Category 2] - e.g., Integrations
3. [Category 3] - e.g., Analytics

**Evaluation Criteria**:
- âœ“ = Full support (complete implementation)
- âš ï¸ = Partial support (limited or basic)
- âŒ = Not available
- ğŸ”œ = Emerging trend (not yet standard)
```

### Step 2: Research Feature Availability
**Goal**: Gather accurate data on feature availability

**Research Methods**:
- Product documentation review
- Competitor website audits
- Free trial testing
- User reviews and feedback
- Industry reports
- Sales team intelligence

**Documentation**:
- Screenshot evidence
- Feature descriptions
- Limitations noted
- Pricing tiers (if applicable)
- Last updated date

### Step 3: Generate Comparison Matrix
**Goal**: Create visual comparison

**Use Matrix Generator Agent**: Invoke `./.claude/agents/matrix-generator.md`

**Matrix Format**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature         â”‚ Us  â”‚Comp1 â”‚Comp2 â”‚Comp3 â”‚Market â”‚Priorityâ”‚
â”‚                 â”‚     â”‚      â”‚      â”‚      â”‚Expect â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Social Login    â”‚ âŒ  â”‚ âœ“    â”‚ âœ“    â”‚ âœ“    â”‚ âœ“     â”‚ P0     â”‚
â”‚ 2FA             â”‚ âš ï¸  â”‚ âœ“    â”‚ âœ“    â”‚ âŒ   â”‚ âœ“     â”‚ P1     â”‚
â”‚ SSO/SAML        â”‚ âŒ  â”‚ âœ“    â”‚ âŒ   â”‚ âœ“    â”‚ âš ï¸    â”‚ P1     â”‚
â”‚ API Access      â”‚ âœ“   â”‚ âœ“    â”‚ âš ï¸   â”‚ âœ“    â”‚ âœ“     â”‚ âœ“      â”‚
â”‚ Webhooks        â”‚ âš ï¸  â”‚ âœ“    â”‚ âŒ   â”‚ âœ“    â”‚ âš ï¸    â”‚ P2     â”‚
â”‚ Mobile App      â”‚ âœ“   â”‚ âœ“    â”‚ âœ“    â”‚ âœ“    â”‚ âœ“     â”‚ âœ“      â”‚
â”‚ AI Features     â”‚ âŒ  â”‚ âŒ   â”‚ âš ï¸   â”‚ âœ“    â”‚ ğŸ”œ    â”‚ P0     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Legend: âœ“ Full  âš ï¸ Partial  âŒ None  ğŸ”œ Emerging             â”‚
â”‚ Gap Analysis: 3 Critical, 2 Important, 1 Nice-to-have       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 4: Analyze and Interpret
**Goal**: Extract insights from comparison

**Gap Analysis**:
- **Critical Gaps**: We lack, all competitors have, market expects
- **Important Gaps**: We lack, some competitors have
- **Nice-to-have**: Emerging features, not yet standard
- **Differentiators**: We have, competitors lack

**Priority Assignment**:
- P0: Critical gaps (competitive disadvantage)
- P1: Important gaps (falling behind)
- P2: Nice-to-have (future consideration)
- âœ“: Parity achieved (maintain)

**Insights**:
- Market positioning implications
- User acquisition/retention impact
- Competitive advantages/disadvantages
- Strategic recommendations

**Output**: `./outputs/decision-matrices/[topic]-feature-comparison-[date].md`

## Workflow 2: Multi-Criteria Decision Matrix (MCDA)

### Use Case
Evaluating multiple options against weighted criteria to identify the best choice.

### Step 1: Define Decision and Options
**Goal**: Clearly state what's being decided and what options exist

**Decision Statement**:
```markdown
## Decision: [Clear statement]
**Context**: [Why this decision is needed]
**Timeline**: [When decision must be made]

## Options
1. **Option A**: [Description]
2. **Option B**: [Description]
3. **Option C**: [Description]
```

### Step 2: Identify Evaluation Criteria
**Goal**: Determine what factors matter for this decision

**Common Criteria Categories**:
- **User Impact**: Does it solve real user problems?
- **Business Value**: Revenue, retention, strategic alignment
- **Feasibility**: Technical complexity, resource availability
- **Cost**: Development, maintenance, opportunity cost
- **Time**: Speed to market, time to value
- **Risk**: Technical, market, execution risk
- **Strategic Fit**: Aligns with vision and roadmap

**Criteria Selection**:
- Choose 5-7 criteria (not too many)
- Make criteria independent (no overlap)
- Define each criterion clearly
- Include both benefits and costs

**Example**:
```markdown
## Evaluation Criteria

1. **User Impact** (Does it solve a key user problem?)
   - 10/10: Solves critical pain point for all users
   - 5/10: Solves moderate problem for some users
   - 1/10: Minimal user benefit

2. **Feasibility** (Can we build it with current resources?)
   - 10/10: Easy, low complexity, clear path
   - 5/10: Moderate complexity, some unknowns
   - 1/10: Very complex, high uncertainty

[Define all criteria similarly]
```

### Step 3: Assign Weights
**Goal**: Reflect relative importance of each criterion

**Weighting Approaches**:
- **Equal Weights**: All criteria matter equally (use when uncertain)
- **Strategic Weights**: Align with company priorities
- **Stakeholder Weights**: Reflect stakeholder consensus
- **Data-Driven Weights**: Based on historical decisions

**Example Weights**:
```markdown
## Criteria Weights
| Criterion | Weight | Rationale |
|-----------|--------|-----------|
| User Impact | 30% | Primary goal is user satisfaction |
| Feasibility | 25% | Resource constrained |
| Business Value | 20% | Revenue important but secondary |
| Risk | 15% | Must manage downside |
| Time to Market | 10% | Some urgency but not critical |
| **Total** | **100%** | |
```

**Important**: Set weights BEFORE scoring options (avoid bias).

### Step 4: Score Options
**Goal**: Evaluate each option against each criterion

**Scoring Guidelines**:
- Use consistent scale (e.g., 1-10)
- Score independently (don't compare during scoring)
- Use evidence/data where possible
- Document reasoning
- Consider multiple perspectives

**Scoring Session**:
- Involve relevant experts
- Score one criterion at a time across all options
- Discuss and align on scores
- Document assumptions

### Step 5: Create Decision Matrix
**Goal**: Calculate weighted scores and identify winner

**Use Matrix Generator Agent**: Invoke `./.claude/agents/matrix-generator.md`

**Matrix Format**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Solution      â”‚ Cost   â”‚ Time    â”‚ Risk    â”‚ Impact  â”‚Total â”‚
â”‚               â”‚ (20%)  â”‚ (25%)   â”‚ (25%)   â”‚ (30%)   â”‚Score â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Build In-Houseâ”‚ 3/10   â”‚ 2/10    â”‚ 8/10    â”‚ 10/10   â”‚ 6.4  â”‚
â”‚               â”‚ $200K  â”‚ 6 monthsâ”‚ Low riskâ”‚ Full ctlâ”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Buy Solution  â”‚ 7/10   â”‚ 9/10    â”‚ 5/10    â”‚ 7/10    â”‚ 7.0  â”‚
â”‚               â”‚ $50K   â”‚ 1 month â”‚ Med riskâ”‚ 80% fit â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Partner/Integ â”‚ 9/10   â”‚ 7/10    â”‚ 3/10    â”‚ 6/10    â”‚ 6.2  â”‚
â”‚               â”‚ $10K   â”‚ 2 monthsâ”‚High riskâ”‚ 60% fit â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Hybrid        â”‚ 6/10   â”‚ 5/10    â”‚ 6/10    â”‚ 9/10    â”‚ 7.1ğŸ†â”‚
â”‚               â”‚ $75K   â”‚ 3 monthsâ”‚ Med riskâ”‚ 90% fit â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

Calculation: Score = Î£(Criterion Weight Ã— Option Score)
Recommendation: Hybrid Approach (Score: 7.1)
Rationale: Best balance of control, speed, and impact
```

### Step 6: Sensitivity Analysis
**Goal**: Test robustness of decision

**Questions to Ask**:
- What if weights were different?
- What if scores changed by Â±1 point?
- What assumptions is this sensitive to?
- Is there a clear winner or is it close?

**Sensitivity Test**:
- Adjust weights Â±10%
- Adjust scores Â±1 point
- Rerun calculation
- If winner changes â†’ decision is fragile
- If winner stays same â†’ decision is robust

### Step 7: Document Decision
**Goal**: Record rationale and recommendation

**Decision Documentation**:
```markdown
## Decision Matrix Analysis: [Topic]

**Recommendation**: [Winning option]
**Score**: X.X/10
**Confidence**: [High/Medium/Low]

**Why This Option**:
1. [Strength 1]
2. [Strength 2]
3. [Strength 3]

**Trade-offs Accepted**:
- [Trade-off 1]
- [Trade-off 2]

**Sensitivity**: [Robust/Fragile]
- Tested with weight variations: [Result]
- Tested with score variations: [Result]

**Assumptions**:
- [Assumption 1]
- [Assumption 2]

**Next Steps**:
- [ ] Action 1
- [ ] Action 2
```

**Output**: `./outputs/decision-matrices/[topic]-mcda-[date].md`

## Workflow 3: Research Synthesis Matrix

### Use Case
Synthesizing findings from multiple research sources to assess confidence.

### Step 1: Identify Research Sources
**Goal**: List all sources that inform the finding

**Source Types**:
- User interviews (qualitative)
- Surveys (quantitative)
- Analytics (behavioral)
- Competitor analysis (market)
- Expert opinions (consultation)
- Support tickets (voice of customer)

### Step 2: Extract Findings per Source
**Goal**: Document what each source says about key topics

**Extraction Template**:
```markdown
## Source: [Name]
**Type**: [Qualitative/Quantitative/Mixed]
**Sample**: n=X
**Date**: [YYYY-MM-DD]

### Finding on [Topic]
**Evidence**: [Quote, stat, or observation]
**Strength**: [Strong/Medium/Weak]
**Context**: [Important context]
```

### Step 3: Create Synthesis Matrix
**Goal**: Compare findings across sources

**Use Matrix Generator Agent**: Invoke `./.claude/agents/matrix-generator.md`

**Matrix Format**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ Finding      â”‚Interview â”‚Analytics â”‚Competitorâ”‚ Survey â”‚Conf.â”‚
â”‚              â”‚ (n=20)   â”‚ (n=10K)  â”‚ Analysis â”‚ (n=500)â”‚Scoreâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ Need faster  â”‚ 18/20    â”‚ 3s avg   â”‚ All <2s  â”‚78% agreeâ”‚ 9.2 â”‚
â”‚ load times   â”‚ mentionedâ”‚ load timeâ”‚          â”‚         â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ Want dark    â”‚ 5/20     â”‚ 22% nightâ”‚ 3/5 have â”‚45% wantâ”‚ 5.8 â”‚
â”‚ mode         â”‚mentioned â”‚ usage    â”‚          â”‚         â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ Mobile first â”‚ 15/20    â”‚ 67%      â”‚ All mobileâ”‚71%    â”‚ 8.9 â”‚
â”‚ priority     â”‚ mobile   â”‚ traffic  â”‚optimized â”‚mobile   â”‚     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Confidence Score Calculation:
- 4 converging sources: 9-10 (Very High)
- 3 converging sources: 7-8 (High)
- 2 converging sources: 5-6 (Medium)
- 1 source only: 3-4 (Low)

Action Thresholds:
9-10: Very High â†’ Act immediately
7-8: High â†’ Plan for next quarter
5-6: Medium â†’ Further research needed
<5: Low â†’ Monitor only
```

### Step 4: Analyze Patterns
**Goal**: Identify convergence, divergence, and gaps

**Pattern Analysis**:
- **Converging Evidence**: 3+ sources agree â†’ High confidence
- **Diverging Evidence**: Sources conflict â†’ Investigate why
- **Unique Insights**: Single source â†’ Validate or explore
- **Knowledge Gaps**: No data available â†’ Research needed

### Step 5: Prioritize Insights
**Goal**: Rank findings by confidence and impact

**Impact vs. Evidence Matrix**:
```
         Strong Evidence (7-10)
         â”‚
High     â”‚ PRIORITY 1    â”‚
Impact   â”‚ ACT NOW       â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Low      â”‚ DOCUMENT      â”‚ IGNORE
Impact   â”‚               â”‚
         â”‚               â”‚
        High Impact     Low Impact
```

**Output**: `./outputs/decision-matrices/[topic]-research-synthesis-[date].md`

## Workflow 4: Stakeholder Position Matrix

### Use Case
Visualizing stakeholder alignment for consensus building.

**See**: `./.claude/workflows/consensus-report.md` for full workflow

**Quick Matrix**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Issue         â”‚ Engineeringâ”‚   Design   â”‚  Sales   â”‚Consensus â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Priority      â”‚ P1         â”‚ P0         â”‚ P0       â”‚ P0       â”‚
â”‚ Position      â”‚ Need time  â”‚ Critical UXâ”‚ Deals    â”‚ MVP Q1   â”‚
â”‚ Concern       â”‚ Security   â”‚ Consistencyâ”‚ Pipeline â”‚ Security â”‚
â”‚ Support Level â”‚ Commit     â”‚ Support    â”‚ Support  â”‚ Strong   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Alignment Score: 4.2/5.0 (Strong alignment)
```

## Workflow 5: Prioritization Matrices

### Option A: Impact vs. Effort (2x2)
**Use Case**: Quick prioritization of features/initiatives

**Matrix**:
```
         High Impact
         â”‚
Quick    â”‚  Quick    â”‚  Major
Wins     â”‚  Wins     â”‚  Projects
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Fill-Ins â”‚  Thanklessâ”‚
         â”‚  Tasks    â”‚
         â”‚           Low Impact
        Low Effort    High Effort

Instructions:
1. Plot each feature in appropriate quadrant
2. Prioritize: Quick Wins â†’ Major Projects â†’ Fill-Ins
3. Avoid or defer: Thankless Tasks
```

### Option B: RICE Scoring
**Use Case**: Quantitative prioritization with confidence

**RICE Formula**:
```
RICE Score = (Reach Ã— Impact Ã— Confidence) / Effort

Where:
- Reach: Number of users affected per time period
- Impact: Degree of impact (0.25 = Minimal, 0.5 = Low, 1 = Medium, 2 = High, 3 = Massive)
- Confidence: Certainty level (50% = Low, 80% = Medium, 100% = High)
- Effort: Person-months required
```

**Matrix**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Feature     â”‚Reach â”‚ Impact â”‚Confidenceâ”‚Effortâ”‚Score â”‚
â”‚             â”‚(users)â”‚(0-3)  â”‚  (%)   â”‚(pers-â”‚(RICE)â”‚
â”‚             â”‚       â”‚       â”‚        â”‚mos)  â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ AI Assistantâ”‚10000 â”‚   3    â”‚  80%   â”‚  3   â”‚ 8000 â”‚
â”‚ Dark Mode   â”‚ 5000 â”‚   1    â”‚  90%   â”‚  1   â”‚ 4500 â”‚
â”‚ API v2      â”‚ 2000 â”‚   2    â”‚  60%   â”‚  2   â”‚ 1200 â”‚
â”‚ Mobile App  â”‚ 8000 â”‚   2    â”‚  70%   â”‚  4   â”‚ 2800 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

Priority Order: AI Assistant â†’ Dark Mode â†’ Mobile App â†’ API v2
```

### Option C: Value vs. Complexity
**Use Case**: Strategic prioritization considering strategic value

**Matrix**:
```
High Value â”‚ Table Stakes â”‚ High Value  â”‚
           â”‚ (Do)         â”‚ (Prioritize)â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Low Value  â”‚ Don't Do     â”‚ Low-Hanging â”‚
           â”‚              â”‚ Fruit (Do)  â”‚
           â”‚              â”‚             â”‚
          Low Complexity  High Complexity
```

## Best Practices

### General Matrix Creation
1. **Define purpose first**: What decision is this matrix supporting?
2. **Choose appropriate type**: Match matrix to decision type
3. **Keep it simple**: 5-7 criteria/dimensions maximum
4. **Use consistent scales**: Don't mix 1-5 and 1-10 scales
5. **Document methodology**: How you scored/rated
6. **Add context**: Legends, notes, caveats
7. **Make actionable**: Clear recommendations
8. **Update regularly**: Matrices become stale

### Visual Design
- Use clear visual separators (lines, spacing)
- Add color coding when possible (ğŸŸ¢ğŸŸ¡ğŸ”´)
- Include legends and keys
- Highlight winners/priorities
- Make scannable (bold important numbers)
- Use icons/symbols (âœ“/âš /âŒ)

### Data Quality
- Verify data accuracy
- Document sources
- Note confidence levels
- Acknowledge gaps
- Update when stale
- Version control

### Stakeholder Communication
- Lead with insights, not the matrix
- Explain methodology clearly
- Show sensitivity analysis
- Address concerns proactively
- Make recommendation explicit
- Provide next steps

## Validation Checklist

### Structure
- [ ] Purpose clearly stated
- [ ] Appropriate type selected
- [ ] Dimensions logically organized
- [ ] All cells populated
- [ ] Formatting consistent

### Data
- [ ] Data accurate and current
- [ ] Sources documented
- [ ] Scoring/rating consistent
- [ ] No obvious errors
- [ ] Gaps acknowledged

### Analysis
- [ ] Calculations correct
- [ ] Weights justified
- [ ] Scores defensible
- [ ] Patterns accurate
- [ ] Conclusions supported

### Usability
- [ ] Easy to understand quickly
- [ ] Key insights visible
- [ ] Legend/key provided
- [ ] Actionable recommendations
- [ ] Appropriate for audience

## Integration Points

**Receives input from**:
- Research Synthesizer (data for matrices)
- Consensus Builder (stakeholder positions)
- Prioritization Engine (scoring criteria)
- Analytics Synthesizer (metrics, trends)

**Feeds into**:
- Consensus Reports (visual frameworks)
- PRDs (requirements prioritization)
- Strategic Planning (competitive positioning)
- Stakeholder Communication (decision artifacts)

## Tools and Templates

- Matrix Generator Agent: `./.claude/agents/matrix-generator.md`
- Feature Comparison Template: `./.claude/templates/feature-comparison-matrix.md`
- MCDA Template: `./.claude/templates/decision-matrix.md`
- Research Synthesis Template: `./.claude/templates/research-matrix.md`

## Related Workflows

- Research Synthesis: `./.claude/workflows/research-synthesis.md`
- Consensus Building: `./.claude/workflows/consensus-report.md`
- Feature Prioritization: `./.claude/workflows/feature-prioritization.md`
- Problem Decomposition: `./.claude/workflows/problem-decomposition.md`
